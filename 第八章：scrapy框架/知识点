scrapy框架

- 什么是框架
    - 就是一个集成了很多功能并且具有很强通用性的一个项目模板
- 如何学习框架
    - 专业学习框架封装的各种功能的详细用法
- 什么是scrapy
    - 爬虫中封装好的一个明星框架。功能：高性能的持久化存储，异步的数据下载，高性能的数据解析，分布式

- scrapy框架的基本使用
    - 环境安装：
    - mac or linux：pip install scrapy
    - windows:
        - pip install wheel
        - 下载twisted，下载地址为：http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted
        - 安装twisted,pip install twisted
        - pip intsall pywin32
        - pip install scrapy
        测试：在终端里录入scrapy指令，没有报错即表示安装成功
        以上这些都可以在pycharm中，或者pip中进行下载安装。
    - 创建一个工程：scrapy startproject xxxPro
    - cd xxxPro
    - 在spiders子目录中创建一个爬虫文件
        - scrapy genspider spiderName www.xxx.com (创建的爬虫文件会自动的放到spiders文件中)
    - 执行工程：
        - scrapy crawl spiderName

- scrapy数据解析

- scrapy持久化存储
    - 基于终端指令：
        - 要求：只可以将parse方法的返回值存储到本地的文本文件中
        - 注意：持久化存储对应的文本文件的类型只可以为：('json', 'jsonlines', 'jsonl', 'jl', 'csv', 'xml', 'marshal','pickle')
        - 指令：scrapy crawl spiderName -o filepath
        - 好处：简洁高效便捷
        - 缺点：局限性比较强（数据只可以存储到指定后缀的文本文件中）
    - 基于管道：
        - 编码流程：
            - 数据解析
            - 在item类中定义相关的属性
            - 将解析的数据封装存储到item类型的对象
            - 将 item类型的对象提交给管道进行持久化存储的操作
            - 在管道类的process_item中要将其接受的item对象中存储的数据进行持久化存储操作
            - 在配置文件中开启管道
        - 好处：
            - 通用性强。
            - 编码流程复杂

        - 面试题：将爬取到的数据一份存储到本地一份存储到数据库，如何实现？
            - 管道文件中一个管道类对应的是将数据存储到一种平台
            - 爬虫文件提交的item只会给管道文件中第一个被执行的管道类接收
            - process_item中的  return item  表示将item传递给下一个即将被执行的管道类

- 基于spider的全站数据爬取
    - 就是将网站中某板块下的全部页码对应的页面数据进行爬取
    - 需求：爬取昵图网美女板块中照片的名称
    - 实现方式：
        - 将所有页面的url添加到start_url列表中（不推荐）
        - 自行手动的进行请求发送
        - yield scrapy.Request(url,callback):callback专门用于数据解析

- 五大核心组件
    引擎（Scrapy）
        用来处理整个系统的数据流处理，触发事务（框架核心）
    调度器（Scheduler）
        用来接收引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回，可以想象成一个url（抓取网页）
    下载器（Downloader）
        用于下载网页内容，并将网页内容返回给蜘蛛（Scrapy下载器是建立在twisted这个高效的异步模型上）
    爬虫（spiders）
        爬虫是主要干活的，用于从特定的网页中提取自己需要的信息，及所谓的实体（Item）。用户也可以从中提取数据
    项目管道（Pipeline）
        负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体，验证实体的有效性，清楚不需要的信息。

    - 请求传参
        - 使用场景：如果爬取解析的数据不再同一张页面中。（深度爬取）

    - 图片数据爬取值ImagesPipeline
        - 基于scrapy爬取字符串类型的数据和爬取图片类型的数据区别？
            - 字符串：只需要基于xpath进行解析并且提交管道进行持久化存储
            - 图片：xpath解析出图片src的属性值。单独对图片地址发起请求获取图片的二进制数据

        - ImagesPipeline：
            - 只需要将img色src的属性值进行解析，提交给管道，管道就会对图片的src进行请求发送获取图片的二进制数据
       需求：爬取站长素材的美女高清图片
        - 使用流程：
            - 数据解析（图片的地址）
            - 将存储图片地址的item提交给制定的管道类
            - 在管道文件中自定制一个基于ImagesPipeline的一个管道类
                - get_media_request
                - file_path
                - itme_completed
            - 在配置文件中：
                - 指定图片存储的目录：IMAGES_STORE = './imgs'
                - 指定开启的管道：自定制的管道类

- 中间件
    - 下载中间件
        - 位置：引擎和下载器之间
        - 作用：批量拦截到整个工程中所有的请求和响应
        - 拦截请求：
            - UA伪装
            - 代理IP
        - 拦截响应：
            - 篡改响应数据，响应对象
            - 需求：爬取网易新闻中的新闻数据（标题和内容）
                - 1.通过网易新闻的首页解析出五大板块对应的详情页的url（没有动态加载）
                - 2.每一个板块对应的新闻标题都是动态加载出来的（动态加载）其实网页新闻的标题并不是动态加载的，这里只是学习思路
                - 3.通过解析出每一条新闻详情页的url获取详情页的页面源码，解析出新闻内容

CrawlSpider:类，Spider的一个子类
    - 全站数据爬取的方式
        - 基于Spider：手动请求
        - 基于CrawlSpider
    CrawlSpider的使用：
        - 创建一个工程
        - cd xxx
        - 创建爬虫文件（CrawlSpider）：
            - scrapy genspider -t crawl xxx www.xxx.com
            - 连接提取器：
                - 作用：根据指定规则（allow）进行指定连接的提取
            - 规则解析器：
                - 作用：将连接提取器提取到的链接进行指定规则（callback）的解析

        #需求：爬取sun网站中的编号，新闻标题，新闻内容，标号
            - 分析：爬取的数据没有在同一张页面中。



- 分布式爬虫
    - 概念：我们需要搭建一个分布式的机群，让其对一组资源进行分布联合爬取。
    - 作用：提升爬取数据的效率

    - 如何实现分布式？
        - 安装一个scrapy-redis的组件
        - 原生的scrapy是不可以实现分布式爬虫，必须要让scrapy结合着scrapy-redis组件一起实现分布式爬虫
        - 为什么原生的scrapy实现不了分布式爬虫
            - 调度器不可以被分布式机群共享
            - 管道不可以被分布式机群共享
        - scrapy-redis组件作用：
            - 可以给原生的scrapy框架提供可以被共享的管道和调度器





